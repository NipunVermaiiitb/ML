import pandas as pd
import numpy as np
from sklearn.model_selection import KFold, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# ================== LOAD TRAIN DATA ==================
train = pd.read_csv('/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/train.csv')
target_col = 'Transport_Cost'

# ================== COMPUTE DAYS_BETWEEN_ORDER_DELIVERY ==================
train['Order_Placed_Date'] = pd.to_datetime(train['Order_Placed_Date'], format='%m/%d/%y', errors='coerce')
train['Delivery_Date'] = pd.to_datetime(train['Delivery_Date'], format='%m/%d/%y', errors='coerce')
train['Days_Between_Order_Delivery'] = (train['Delivery_Date'] - train['Order_Placed_Date']).dt.days

# Keep only rows with valid Transport_Cost (no negatives)
# train = train[train[target_col] >= 0]

# Replace negative or missing Days_Between_Order_Delivery with median
median_days = train['Days_Between_Order_Delivery'].median()
train.loc[train['Days_Between_Order_Delivery'] < 0, 'Days_Between_Order_Delivery'] = median_days
train['Days_Between_Order_Delivery'] = train['Days_Between_Order_Delivery'].fillna(median_days).astype(int)
# ================== DROP UNNECESSARY COLUMNS ==================
drop_cols = ['Hospital_Id', 'Hospital_Location', 'Order_Placed_Date', 'Delivery_Date', 'Supplier_Name']
train = train.drop(columns=drop_cols, errors='ignore')

# ================== SEPARATE FEATURES AND TARGET ==================
cols_to_drop = [  
    "Hospital_Info",
    "Fragile_Equipment",
    "Days_Between_Order_Delivery",
    "Rural_Hospital"
]     # Least weighted features according to Lasso

# Drop the columns
X = train.drop(columns=cols_to_drop+[target_col])
y = train[target_col]

# ================== IMPUTATION ==================
numerical_cols = X.select_dtypes(include=np.number).columns.tolist()
categorical_cols = X.select_dtypes(exclude=np.number).columns.tolist()
# print(X[categorical_cols].head(20))

num_imputer = SimpleImputer(strategy='median')
cat_imputer = SimpleImputer(strategy='most_frequent')
X[numerical_cols] = num_imputer.fit_transform(X[numerical_cols])
X[categorical_cols] = cat_imputer.fit_transform(X[categorical_cols])

# ================== OUTLIER REMOVAL (NO SCALER) ==================
Q1 = X[numerical_cols].quantile(0.25)
Q3 = X[numerical_cols].quantile(0.75)
IQR = Q3 - Q1
lower_bounds = Q1 - 0.7 * IQR
upper_bounds = Q3 + 2.1 * IQR

mask_no_outliers = ~((X[numerical_cols] < lower_bounds) | (X[numerical_cols] > upper_bounds)).any(axis=1)

# Filter y outliers using wide IQR method
Q1_y, Q3_y = y.quantile(0.10), y.quantile(0.90)
IQR_y = Q3_y - Q1_y
lower_y = Q1_y - 50 * IQR_y
upper_y = Q3_y + 125 * IQR_y
mask_y = (y >= lower_y) & (y <= upper_y)

mask = mask_no_outliers & mask_y
X = X[mask]
y = y[mask]

print(f"After outlier removal (no scaling): {len(y)} rows kept")
# ================== TARGET ENCODING ==================
print("\nApplying target encoding...")

target_encodings = {}
global_mean = y.mean()

for col in categorical_cols:
    means = y.groupby(X[col]).mean()
    target_encodings[col] = means
    X[col] = X[col].map(means)
    X[col] = X[col].fillna(global_mean)

print("Target encoding applied to training data.")

# # ================== LABEL ENCODING ==================
# onehot_cols = [
#     'Equipment_Type',
#     'CrossBorder_Shipping',
#     'Urgent_Shipping',
#     'Installation_Service',
#     'Transport_Method',
#     'Fragile_Equipment',
#     'Rural_Hospital'
# ]

# # Columns that are ordinal → label encoding
# # For example, Hospital_Info: Working Class < Wealthy
# label_cols = ['Hospital_Info']

# # ================= One-Hot Encoding =================
# X = pd.get_dummies(X, columns=onehot_cols, drop_first=True)

# # ================= Label Encoding =================
# label_encoders = {}
# for col in label_cols:
#     le = LabelEncoder()
#     X[col] = le.fit_transform(X[col].astype(str))
#     label_encoders[col] = le

# Check results
# print(X.head())

# ================== RANDOM FOREST OR XGB CHOICE ==================
use_model = "rf"

if use_model == "rf":
    base_model = RandomForestRegressor(random_state=42, n_jobs=-1)
    param_grid = {
        "lasso__n_estimators": [100, 200, 300, 500],
        "lasso__max_depth": [None, 10, 20, 30],
        "lasso__min_samples_split": [2, 5, 10],
        "lasso__min_samples_leaf": [1, 2, 4],
        "lasso__max_features": ["sqrt", "log2", None],
    }
else:
    base_model = XGBRegressor(
        random_state=42,
        n_jobs=-1,
        objective="reg:squarederror",
        tree_method="hist",
        eval_metric="rmse",
        verbosity=0
    )
    param_grid = {
        "lasso__n_estimators": [200, 400, 600],
        "lasso__learning_rate": [0.01, 0.05, 0.1],
        "lasso__max_depth": [3, 5, 7, 9],
        "lasso__subsample": [0.6, 0.8, 1.0],
        "lasso__colsample_bytree": [0.6, 0.8, 1.0],
    }

# ================== PIPELINE ==================
lasso_pipeline = Pipeline([
    ('lasso', base_model)
])

# ================== HYPERPARAMETER TUNING ==================
search = RandomizedSearchCV(
    estimator=lasso_pipeline,
    param_distributions=param_grid,
    n_iter=15,               # reduce if slow
    cv=3,
    scoring="r2",
    random_state=42,
    n_jobs=-1,
    verbose=2
)



# scaler1 = StandardScaler()
# X_scaled = scaler1.fit_transform(X)
# X = pd.DataFrame(X_scaled, columns = X.columns)

# ================== K-FOLD VALIDATION ==================
kf = KFold(n_splits=2, shuffle=True, random_state=42)
rmse_list, r2_list, adj_r2_list = [], [], []
n = len(X)
p = X.shape[1]
for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    search.fit(X_train, y_train)
    best_model = search.best_estimator_

    y_val_pred = best_model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))
    r2 = r2_score(y_val, y_val_pred)
    
    n_val = len(y_val)
    adj_r2 = 1 - (1 - r2) * ((n_val - 1) / (n_val - p - 1))

    rmse_list.append(rmse)
    r2_list.append(r2)
    adj_r2_list.append(adj_r2)

    print(f"Fold {fold}: RMSE = {rmse:.3f}, R² = {r2:.3f}, Adjusted R² = {adj_r2:.3f}")
    print(f"Best Params for Fold {fold}: {search.best_params_}\n")

print(f"\nAverage RMSE: {np.mean(rmse_list):.3f}")
print(f"Average R²: {np.mean(r2_list):.3f}")
print(f"Average Adjusted R²: {np.mean(adj_r2_list):.3f}")

# ================== FINAL TRAIN ON FULL DATA ==================
best_model_final = search.best_estimator_
best_model_final.fit(X, y)

print(f"\nBest Overall Params: {search.best_params_}")



# ================== LOAD TEST SET ==================
test = pd.read_csv('/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/test.csv')
hospital_ids = test['Hospital_Id'].copy()

test['Order_Placed_Date'] = pd.to_datetime(test['Order_Placed_Date'], format='%m/%d/%y', errors='coerce')
test['Delivery_Date'] = pd.to_datetime(test['Delivery_Date'], format='%m/%d/%y', errors='coerce')
test['Days_Between_Order_Delivery'] = (test['Delivery_Date'] - test['Order_Placed_Date']).dt.days

test.loc[test['Days_Between_Order_Delivery'] < 0, 'Days_Between_Order_Delivery'] = median_days
test['Days_Between_Order_Delivery'] = test['Days_Between_Order_Delivery'].fillna(median_days).astype(int)

test = test.drop(columns=drop_cols, errors='ignore')

test[numerical_cols] = num_imputer.transform(test[numerical_cols])
test[categorical_cols] = cat_imputer.transform(test[categorical_cols])

# Apply target encoding to test set using training means
for col in categorical_cols:
    means = target_encodings[col]
    test[col] = test[col].map(means)
    test[col] = test[col].fillna(global_mean)

# Ensure test has same columns as train
test = test.reindex(columns=X.columns, fill_value=0)
# test = scaler1.transform(test)

# ================== PREDICTION ==================
y_pred_test = best_model_final.predict(test)
pred_df = pd.DataFrame({
    'Hospital_Id': hospital_ids,
    'Transport_Cost': y_pred_test
})

print("\n===== Sample Predictions =====")
print(pred_df.head())
pred_df.to_csv('submission.csv', index=False)
print(f"\nSubmission file created with {len(pred_df)} predictions.")
